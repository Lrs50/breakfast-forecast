import streamlit as st

st.set_page_config(page_title="Previs√£o dos Itens do Caf√© da Manh√£", page_icon="üìä", layout="wide")

col = st.columns([3, 2])

with col[0]:
    text = """
    ### Tecnologias e Arquitetura do Projeto

    Este projeto tem como objetivo construir uma solu√ß√£o completa para a **coleta**, **processamento**, **an√°lise**, **previs√£o** e **disponibiliza√ß√£o de dados** relacionados ao consumo de itens do caf√© da manh√£. A seguir, explico os principais componentes e como eles se integram na arquitetura geral do sistema.

    O **Data Warehouse**, implementado em **MySQL**, √© o reposit√≥rio central onde todos os dados processados s√£o armazenados. Ele √© alimentado por pipelines de **ETL** orquestradas com **Apache Airflow**, que rodam em cont√™ineres **Docker**, garantindo escalabilidade e reprodutibilidade ao sistema.

    As ETLs integram dados de duas principais fontes:
    - A **API SIDRA do IBGE**, que fornece s√©ries hist√≥ricas de pre√ßos de alimentos.
    - Plataformas de supermercados e delivery, como a **UberEats**, atrav√©s de **web scraping** com **Selenium** e **BeautifulSoup**.
    """
    st.markdown(text)

with col[1]:    
    st.write(" ")
    st.write(" ")
    st.image("imgs/diagrama_dados_breakfast.png", caption="Diagrama do Projeto")

text =  """ 
Ap√≥s a coleta, os dados s√£o transformados e tratados com as bibliotecas **Pandas** e **NumPy**, e posteriormente armazenados no MySQL. Para facilitar o acesso externo e modularizar a solu√ß√£o, foi desenvolvida uma **API com FastAPI** que permite consultas seguras e r√°pidas aos dados.

Na etapa de modelagem, o sistema utiliza **Facebook Prophet** para previs√£o de s√©ries temporais, complementado por **algoritmos gen√©ticos** que otimizam os par√¢metros dos modelos. A avalia√ß√£o dos resultados √© feita com m√©tricas do **Scikit-learn**, e as previs√µes geradas tamb√©m s√£o armazenadas no Data Warehouse.

A fase inicial de desenvolvimento contou com ampla **prototipagem em Jupyter Notebooks**, acelerando a experimenta√ß√£o e valida√ß√£o de hip√≥teses.

A camada de **visualiza√ß√£o e intera√ß√£o** com o usu√°rio foi constru√≠da utilizando **Streamlit**, que atua tanto como front-end quanto back-end da aplica√ß√£o. A interface permite a explora√ß√£o interativa dos dados, incluindo gr√°ficos de pre√ßos, an√°lises de sazonalidade e previs√µes.

Para enriquecer a experi√™ncia do usu√°rio, a aplica√ß√£o tamb√©m se conecta √† **API do Google Gemini**, respons√°vel por **gerar sugest√µes autom√°ticas de receitas** com base nos ingredientes com tend√™ncia de queda nos pre√ßos ‚Äî uma funcionalidade √∫til e criativa.

Al√©m disso, os dados utilizados nos modelos e visualiza√ß√µes tamb√©m s√£o armazenados em um **Data Lake na Google Cloud Platform**, que serve como reposit√≥rio intermedi√°rio para datasets brutos e processados.

Durante o desenvolvimento, foram utilizadas as seguintes **linguagens de programa√ß√£o**:
- **Python**, principal linguagem para scripts, modelagem preditiva, automa√ß√µes e APIs.
- **SQL**, para manipula√ß√£o e consulta de dados no Data Warehouse.
- **HTML e CSS**, aplicados na personaliza√ß√£o visual da interface Streamlit.

---

Se voc√™ gostou do projeto, tem sugest√µes ou quer trocar ideias sobre dados, entre em contato por **lucaspook12@gmail.com** ou [LinkedIn](https://www.linkedin.com/in/lucas-dos-reis-lrs).
"""

st.markdown(text)
